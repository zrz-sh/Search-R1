{
    "instance_id": "ws_en_040",
    "score": 0.0,
    "precision_by_row": 0.1111111111111111,
    "recall_by_row": 0.03333333333333333,
    "f1_by_row": 0.05128205128205128,
    "precision_by_item": 0.6190476190476191,
    "recall_by_item": 0.18571428571428572,
    "f1_by_item": 0.28571428571428575,
    "msg": "   year_exact_match  rank_exact_match  app_llm_judge  downloads(million)_number_near  category_llm_judge  parentcompany_llm_judge  country_llm_judge\n0               1.0               1.0              1                             0.0                   1                        1                  1\n1               1.0               1.0              0                             0.0                   0                        0                  0\n2               1.0               1.0              0                             0.0                   0                        1                  1\n3               1.0               1.0              1                             0.0                   1                        1                  1\n4               1.0               1.0              0                             0.0                   0                        0                  0\n5               1.0               1.0              0                             0.0                   0                        1                  1\n6               1.0               1.0              1                             1.0                   1                        1                  1\n7               1.0               1.0              0                             1.0                   0                        0                  0\n8               1.0               1.0              1                             0.0                   0                        1                  1"
}