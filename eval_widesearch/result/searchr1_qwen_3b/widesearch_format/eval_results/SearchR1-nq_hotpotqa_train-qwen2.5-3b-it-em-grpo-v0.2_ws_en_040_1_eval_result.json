{
    "instance_id": "ws_en_040",
    "score": 0.0,
    "precision_by_row": 0.0,
    "recall_by_row": 0.0,
    "f1_by_row": 0.0,
    "precision_by_item": 0.5714285714285714,
    "recall_by_item": 0.19047619047619047,
    "f1_by_item": 0.2857142857142857,
    "msg": "   year_exact_match  rank_exact_match  app_llm_judge  downloads(million)_number_near  category_llm_judge  parentcompany_llm_judge  country_llm_judge\n0               1.0               1.0              1                             0.0                   1                        1                  1\n1               1.0               1.0              0                             0.0                   0                        1                  1\n2               1.0               1.0              1                             0.0                   1                        1                  1\n3               1.0               1.0              0                             0.0                   0                        1                  1\n4               1.0               1.0              0                             0.0                   0                        0                  1\n5               1.0               1.0              0                             0.0                   0                        0                  1\n6               1.0               1.0              0                             0.0                   0                        0                  1\n7               1.0               1.0              0                             0.0                   0                        0                  0\n8               1.0               1.0              0                             0.0                   0                        0                  1\n9               1.0               1.0              1                             0.0                   1                        1                  1"
}