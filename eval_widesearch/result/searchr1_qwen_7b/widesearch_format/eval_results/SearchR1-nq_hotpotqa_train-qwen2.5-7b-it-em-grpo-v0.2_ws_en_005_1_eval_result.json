{
    "instance_id": "ws_en_005",
    "score": 0.0,
    "precision_by_row": 0.2727272727272727,
    "recall_by_row": 0.1875,
    "f1_by_row": 0.2222222222222222,
    "precision_by_item": 0.6818181818181818,
    "recall_by_item": 0.46875,
    "f1_by_item": 0.5555555555555556,
    "msg": "   country_llm_judge  university_exact_match  alliance_llm_judge  minimumgparequirement_llm_judge\n0                  1                     1.0                   1                                0\n1                  1                     1.0                   1                                0\n2                  1                     1.0                   1                                0\n3                  1                     1.0                   1                                1\n4                  1                     1.0                   1                                1\n5                  1                     1.0                   1                                1\n6                  1                     1.0                   1                                0\n7                  1                     1.0                   1                                0\n8                  1                     1.0                   1                                0"
}