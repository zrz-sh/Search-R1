{
    "instance_id": "ws_en_005",
    "score": 0.0,
    "precision_by_row": 0.0,
    "recall_by_row": 0.0,
    "f1_by_row": 0.0,
    "precision_by_item": 0.75,
    "recall_by_item": 0.609375,
    "f1_by_item": 0.6724137931034483,
    "msg": "    country_llm_judge  university_exact_match  alliance_llm_judge  minimumgparequirement_llm_judge\n0                   1                     1.0                   1                                0\n1                   1                     1.0                   1                                0\n2                   1                     1.0                   1                                0\n3                   1                     1.0                   1                                0\n4                   1                     1.0                   1                                0\n5                   1                     1.0                   1                                0\n6                   1                     1.0                   1                                0\n7                   1                     1.0                   1                                0\n8                   1                     1.0                   1                                0\n9                   1                     1.0                   1                                0\n10                  1                     1.0                   1                                0\n11                  1                     1.0                   1                                0\n12                  1                     1.0                   1                                0"
}